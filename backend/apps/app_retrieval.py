import os
import uvicorn
import logging
import requests

from fastapi import FastAPI, HTTPException, Depends, Request
from fastapi.responses import JSONResponse
from fastapi import status
from dotenv import load_dotenv
from typing import List, Dict, Any

from backend.src.RAG.retrieval_engine import RetrievalEngine
from backend.src.RAG.query_generator import ResearchQueryGenerator
from backend.src.RAG.utils import clean_search_query
from backend.src.backend.pydantic_models import ResearchPaperQuery
from backend.src.constants import ENDPOINT_URLS
from backend.src.backend.user_authentication.utils import validate_request,verify_token
import traceback 

load_dotenv()

app = FastAPI(title="Research Assistant API")
logger = logging.getLogger('uvicorn.error')

if "OPENAI_API_KEY" not in os.environ:
    raise EnvironmentError("openai key not set in environment.")

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
if not OPENAI_API_KEY:
    raise EnvironmentError("openai key not set in environment.")

query_generator = ResearchQueryGenerator(openai_api_key=OPENAI_API_KEY,session_id="foo")
retrieval_engine = RetrievalEngine(openai_api_key=OPENAI_API_KEY)

DATA_INGESTION_URL = f"http://{ENDPOINT_URLS['data_ingestion']['base_url']}{ENDPOINT_URLS['data_ingestion']['path']}"


def use_fast_pipeline(request:Request, additional_queries:List[str]) -> List[Dict[str, Any]]:
    """
    Helper function to retrieve documents using the fast pipeline.
    - By "fast" pipeline, we mean that we first attempt to retrieve documents
        from the existing database, and if none are found, we ingest new data
        and then retrieve the documents.

    Args:
        request (Request): The request object containing the user query.
        additional_queries (List[str]): The additional queries generated by the query generator.
    """
    # Attempt to retrieve documents the existing database
    logger.info("Attempting to retrieve documents from the existing database")
    responses = retrieval_engine.retrieve(user_queries=additional_queries)

    # Attempt to retrieve documents via data ingestion
    if responses:
        logger.info("Relevant documents found in the existing database")
    else:
        logger.info("No relevant documents found, searching for more documents")

        data_ingestion_result = requests.post(url=DATA_INGESTION_URL, json={"user_queries": additional_queries}, headers=request.headers)
        all_entries = data_ingestion_result.json()["all_entries"]

        logger.info(f"Total number of retrieved entries from data ingestion: {len(all_entries)}")
        
        if len(all_entries) == 0:
            logger.info("No entries could be found for this query, please try to rephrase your query.")
        else:
            docs = retrieval_engine.convert_entries_to_docs(entries=all_entries)
            retrieval_engine.split_and_add_documents(docs=docs) # Add documents to ChromaDB (save)

        # Attempt to retrieve the documents again (should be successful this time)
        responses = retrieval_engine.retrieve(user_queries=additional_queries)
    return responses

def use_specific_pipeline(request:Request, additional_queries:List[str]) -> List[Dict[str, Any]]:
    """
    Helper function to retrieve documents using the specific pipeline.
    - By "specific" pipeline, we mean that we always ingest new data and then
      retrieve the documents.
    - More likely to find documents relevant to the user query, but is slower.

    Args:
        request (Request): The request object containing the user query.
        additional_queries (List[str]): The additional queries generated by the query generator
    """
    logger.info("Searching for relevant documents...")
    data_ingestion_result = requests.post(url=DATA_INGESTION_URL, json={"user_queries": additional_queries}, headers=request.headers)
    all_entries = data_ingestion_result.json()["all_entries"]

    logger.info(f"Total number of retrieved entries from data ingestion: {len(all_entries)}")
    if len(all_entries) == 0:
        logger.info("No entries could be found for this query, please try to rephrase your query.")
    else:
        docs = retrieval_engine.convert_entries_to_docs(entries=all_entries)
        retrieval_engine.split_and_add_documents(docs=docs)
    
    # Attempt to retrieve the documents again (should be successful this time)
    responses = retrieval_engine.retrieve(user_queries=additional_queries)
    return responses

@app.post(
        ENDPOINT_URLS['retrieval']['path'], 
        description="Retrieves documents based on the user query.",
        dependencies=[Depends(validate_request)]
        )
async def retrieve_documents(request:Request, query_request:ResearchPaperQuery) -> JSONResponse:
    """

    Retrieves documents based on the user query either through the
    existing database or by ingesting new data and then retrieving
    the documents.
    
    Args:
        query_request (ResearchPaperQuery): The request containing the user query.
    """
    try:
        logger.info("Successfully called retrieval pipeline endpoint")

        payload = verify_token(request)
        username = payload.get("user_id")
        if not username:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="User ID not found in token."
            )
        
        query_generator.session_id = username
        user_input = query_request.user_query
        mode = query_request.mode
        
        logger.info(f"Using mode: {mode}")

        # Generate additional queries
        additional_queries = query_generator.generate(user_input)
        print(additional_queries)

        if additional_queries == "ERROR":
            print("ERROR")
            return {"responses":"ERROR"}
        
        if mode == "fast":
            responses = use_fast_pipeline(request=request, additional_queries=additional_queries)
        elif mode == "specific":
            responses = use_specific_pipeline(request=request, additional_queries=additional_queries)
        else:
            raise Exception("Invalid mode specified. Please select either 'fast' or 'specific'.")
        
        logger.info(f"Responses: {responses}")
        return JSONResponse(content={"responses": responses}, status_code=status.HTTP_200_OK)
    except Exception as e:
        logger.error(f"Error in retrieval: {traceback.format_exc()}") 
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run("app_retrieval:app", host="0.0.0.0", port=8002, reload=True)